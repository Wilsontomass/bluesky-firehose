{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18479a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Standard libraries ----------------\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# ---------------- Data handling ----------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# ---------------- Machine learning & NLP ----------------\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "# ---------------- Deep learning & Transformers ----------------\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------- Visualization ----------------\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b486f9",
   "metadata": {},
   "source": [
    "Embedding text from Parquet data frames using Gemma300m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345beb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Settings ----------------\n",
    "INPUT_PARQUET = \"output/raw_posts_kafka\"\n",
    "BASE_DIR = \"output/raw_posts_embeddings_gemma\"\n",
    "MODEL_ID = \"google/embeddinggemma-300m\"\n",
    "BATCH_SIZE = 256\n",
    "MAX_SEQ_LEN = 128\n",
    "NORMALIZE = False\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# ---------------- NSFW / filtering ----------------\n",
    "NSFW_KEYWORDS = set([\n",
    "    \"porn\",\"sex\",\"nude\",\"sexy\",\"fuck\",\"cock\",\"cum\",\"blowjob\",\n",
    "    \"dick\",\"tits\",\"ass\",\"horny\",\"slut\",\"nsfw\",\"onlyfans\"\n",
    "])\n",
    "\n",
    "MULTILINGUAL_STOPWORDS = set([\n",
    "    \"the\",\"and\",\"a\",\"of\",\"in\",\"to\",\"is\",\"it\",\"for\",\"on\",\n",
    "    \"que\",\"de\",\"le\",\"la\",\"el\",\"en\",\"und\",\"der\",\"die\"\n",
    "])\n",
    "\n",
    "URL_PATTERN = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text_words = [w for w in text.split() if w not in MULTILINGUAL_STOPWORDS]\n",
    "    return \" \".join(text_words)\n",
    "\n",
    "def is_meaningful(text):\n",
    "    if not re.search(r\"\\w\", text):\n",
    "        return False\n",
    "    tokens = text.split()\n",
    "    if len(tokens) == 0:\n",
    "        return False\n",
    "    url_count = sum(bool(URL_PATTERN.match(tok)) for tok in tokens)\n",
    "    if url_count / len(tokens) > 0.5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_nsfw(text):\n",
    "    text_lower = text.lower()\n",
    "    return any(kw in text_lower for kw in NSFW_KEYWORDS)\n",
    "\n",
    "# ---------------- Load posts ----------------\n",
    "df = pd.read_parquet(INPUT_PARQUET, columns=[\"did\", \"rkey\", \"text\"])\n",
    "print(f\"Before filtering: {len(df)} posts.\")\n",
    "\n",
    "df[\"post_id\"] = df[\"did\"].astype(str) + \"/\" + df[\"rkey\"].astype(str)\n",
    "df = df[df[\"text\"].notna()]\n",
    "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "df[\"text_clean\"] = df[\"text\"].apply(preprocess)\n",
    "\n",
    "# ---------------- Preprocessing ----------------\n",
    "df = df[df[\"text_clean\"].apply(is_meaningful)]\n",
    "df = df[~df[\"text_clean\"].apply(is_nsfw)]\n",
    "df = df[df[\"text_clean\"].str.len() > 0]\n",
    "\n",
    "print(f\"After filtering: {len(df)} posts remain.\")\n",
    "\n",
    "# ---------------- Load model ----------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(MODEL_ID, device=device,truncate_dim=256).eval()\n",
    "try:\n",
    "    model.max_seq_length = MAX_SEQ_LEN\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------------- Encode embeddings ----------------\n",
    "texts = df[\"text_clean\"].tolist()\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    prompt_name=\"Clustering\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=False,\n",
    "    convert_to_numpy=True\n",
    ").astype(np.float32)\n",
    "\n",
    "# ---------------- Validate embeddings ----------------\n",
    "finite_mask = np.isfinite(embeddings).all(axis=1)\n",
    "if not finite_mask.all():\n",
    "    n_bad = (~finite_mask).sum()\n",
    "    print(f\"Found {n_bad} rows with NaN/Inf; re-encoding just those rowsâ€¦\")\n",
    "    bad_idx = np.where(~finite_mask)[0]\n",
    "    bad_texts = [texts[i] for i in bad_idx]\n",
    "\n",
    "    fixed = model.encode(\n",
    "        bad_texts,\n",
    "        batch_size=max(64, BATCH_SIZE // 2),\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=False,\n",
    "        convert_to_numpy=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    embeddings[bad_idx] = fixed\n",
    "    assert np.isfinite(embeddings).all(), \"Still found NaN/Inf after repair\"\n",
    "\n",
    "if NORMALIZE:\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    nz = norms.squeeze() > 0\n",
    "    embeddings[nz] = embeddings[nz] / norms[nz]\n",
    "\n",
    "# ---------------- Save embeddings ----------------\n",
    "shutil.rmtree(BASE_DIR, ignore_errors=True)\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "emb_list_arrays = [pa.array(row, type=pa.float32()) for row in embeddings]\n",
    "table = pa.table({\n",
    "    \"post_id\": pa.array(df[\"post_id\"].tolist()),\n",
    "    \"text\": pa.array(df[\"text\"].tolist()),\n",
    "    \"text_clean\": pa.array(df[\"text_clean\"].tolist()),\n",
    "    \"embedding\": pa.array(emb_list_arrays, type=pa.list_(pa.float32()))\n",
    "})\n",
    "\n",
    "fmt = ds.ParquetFileFormat()\n",
    "opts = fmt.make_write_options(compression=os.environ.get(\"PARQUET_COMPRESSION\", \"zstd\"))\n",
    "\n",
    "ds.write_dataset(\n",
    "    data=table,\n",
    "    base_dir=BASE_DIR,\n",
    "    format=fmt,\n",
    "    file_options=opts,\n",
    "    existing_data_behavior=\"overwrite_or_ignore\"\n",
    ")\n",
    "\n",
    "print(f\"Wrote dataset to directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebbbda",
   "metadata": {},
   "source": [
    "Cluster the posts and prep for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d80c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings...\n",
      "Normalizing Embeddings...\n",
      "Dimensionality reduction...\n",
      "Embedding Dimension ---> 64D...\n",
      "Dimensionality reduction done in 80.92 seconds\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\"n_jobs value .* overridden to 1 by setting random_state\")\n",
    "BASE_DIR = \"output/raw_posts_embeddings_gemma\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# ---------------- Load embeddings ----------------\n",
    "print(\"Loading Embeddings...\")\n",
    "dataset = ds.dataset(BASE_DIR, format=\"parquet\")\n",
    "table = dataset.to_table(columns=[\"post_id\", \"text\", \"text_clean\", \"embedding\"])\n",
    "\n",
    "post_id = table.column(\"post_id\").to_pylist()\n",
    "texts = table.column(\"text\").to_pylist()\n",
    "texts_clean = table.column(\"text_clean\").to_pylist()\n",
    "embeddings = np.vstack(table.column(\"embedding\").to_pylist()).astype(np.float32)\n",
    "\n",
    "# Normalize embeddings\n",
    "print(\"Normalizing Embeddings...\")\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "nonzero = norms.squeeze() > 0\n",
    "embeddings[nonzero] /= norms[nonzero]\n",
    "\n",
    "#remove comment below if you want dim reduction before clustering or not\n",
    "\n",
    "# ---------------- Dimensionality reduction ----------------\n",
    "\n",
    "print(\"Dimensionality reduction...\")\n",
    "print(\"Embedding Dimension ---> 64D...\")\n",
    "start_time = time.time()\n",
    "reducer_high = umap.UMAP(random_state=2025,n_components=64, metric=\"cosine\")\n",
    "X_64d = reducer_high.fit_transform(embeddings)\n",
    "end_time = time.time()\n",
    "print(f\"Dimensionality reduction done in {end_time - start_time:.2f} seconds\")\n",
    "embeddings = X_64d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1771788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN clustering...\n",
      "Number of embeddings: 56788\n",
      "HDBSCAN clustering done in 74.23 seconds\n",
      "Found 98 clusters\n",
      "Reducing cluster centroids to 2D...\n"
     ]
    }
   ],
   "source": [
    "# ---------------- HDBSCAN clustering ---------------- \n",
    "print(\"HDBSCAN clustering...\")\n",
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "start_time = time.time()\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=40,metric=\"euclidean\",cluster_selection_method=\"eom\")\n",
    "labels = clusterer.fit_predict(embeddings) \n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"HDBSCAN clustering done in {end_time - start_time:.2f} seconds\")\n",
    "# ---------------- Group clusters ----------------\n",
    "clusters = defaultdict(list)\n",
    "for i, c in enumerate(labels):\n",
    "    if c == -1:\n",
    "        continue\n",
    "    clusters[c].append(i)\n",
    "\n",
    "print(f\"Found {len(clusters)} clusters\")\n",
    "# ---------------- Compute cluster centroids in embedding space ----------------\n",
    "cluster_ids = []\n",
    "cluster_centroids = []\n",
    "\n",
    "for c, idxs in clusters.items():\n",
    "    cluster_ids.append(c)\n",
    "    cluster_centroids.append(embeddings[idxs].mean(axis=0))\n",
    "\n",
    "cluster_centroids = np.vstack(cluster_centroids)\n",
    "\n",
    "# ---------------- Reduce centroids to 2D for visualization ----------------\n",
    "print(\"Reducing cluster centroids to 2D...\")\n",
    "reducer_2d = umap.UMAP(random_state=2025,n_components=2,metric=\"cosine\",  min_dist=0.1,spread=1.0,)\n",
    "X_2d_centroids = reducer_2d.fit_transform(cluster_centroids)\n",
    "cluster_positions = {c: X_2d_centroids[i] for i, c in enumerate(cluster_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd89c98",
   "metadata": {},
   "source": [
    "Create labels for clusters using GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21294b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cluster labels with Gemma 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ea4dd09d234777851260744fc171ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- Gemma 2 Cluster Labeling ----------------\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Generating cluster labels with Gemma 2...\")\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model = model.to(\"cuda\")\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfde41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Cluster: 0\n",
      "Cluster: 1\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Cluster: 4\n",
      "Cluster: 5\n",
      "Cluster: 6\n",
      "Cluster: 7\n",
      "Cluster: 8\n",
      "Cluster: 9\n",
      "Cluster: 10\n",
      "Cluster: 11\n",
      "Cluster: 12\n",
      "Cluster: 13\n",
      "Cluster: 14\n",
      "Cluster: 15\n",
      "Cluster: 16\n",
      "Cluster: 17\n",
      "Cluster: 18\n",
      "Cluster: 19\n",
      "Cluster: 20\n",
      "Cluster: 21\n",
      "Cluster: 22\n",
      "Cluster: 23\n",
      "Cluster: 24\n",
      "Cluster: 25\n",
      "Cluster: 26\n",
      "Cluster: 27\n",
      "Cluster: 28\n",
      "Cluster: 29\n",
      "Cluster: 30\n",
      "Cluster: 31\n",
      "Cluster: 32\n",
      "Cluster: 33\n",
      "Cluster: 34\n",
      "Cluster: 35\n",
      "Cluster: 36\n",
      "Cluster: 37\n",
      "Cluster: 38\n",
      "Cluster: 39\n",
      "Cluster: 40\n",
      "Cluster: 41\n",
      "Cluster: 42\n",
      "Cluster: 43\n",
      "Cluster: 44\n",
      "Cluster: 45\n",
      "Cluster: 46\n",
      "Cluster: 47\n",
      "Cluster: 48\n",
      "Cluster: 49\n",
      "Cluster: 50\n",
      "Cluster: 51\n",
      "Cluster: 52\n",
      "Cluster: 53\n",
      "Cluster: 54\n",
      "Cluster: 55\n",
      "Cluster: 56\n",
      "Cluster: 57\n",
      "Cluster: 58\n",
      "Cluster: 59\n",
      "Cluster: 60\n",
      "Cluster: 61\n",
      "Cluster: 62\n",
      "Cluster: 63\n",
      "Cluster: 64\n",
      "Cluster: 65\n",
      "Cluster: 66\n",
      "Cluster: 67\n",
      "Cluster: 68\n",
      "Cluster: 69\n",
      "Cluster: 70\n",
      "Cluster: 71\n",
      "Cluster: 72\n",
      "Cluster: 73\n",
      "Cluster: 74\n",
      "Cluster: 75\n",
      "Cluster: 76\n",
      "Cluster: 77\n",
      "Cluster: 78\n",
      "Cluster: 79\n",
      "Cluster: 80\n",
      "Cluster: 81\n",
      "Cluster: 82\n",
      "Cluster: 83\n",
      "Cluster: 84\n",
      "Cluster: 85\n",
      "Cluster: 86\n",
      "Cluster: 87\n",
      "Cluster: 88\n",
      "Cluster: 89\n",
      "Cluster: 90\n",
      "Cluster: 91\n",
      "Cluster: 92\n",
      "Cluster: 93\n",
      "Cluster: 94\n",
      "Cluster: 95\n",
      "Cluster: 96\n",
      "Cluster: 97\n",
      "Saved 98 clusters with Gemma 2 labels to output\\clusters.pkl\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "def clean_post(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)     # normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def generate_cluster_label(posts, max_new_tokens=16):\n",
    "    cleaned_posts = [clean_post(p) for p in posts[:5]]\n",
    "\n",
    "    prompt = (\n",
    "        \"Summarize the following 10 posts into 1 concise topic of 3â€“6 words in English. If a certain language is very prevelent include that as first word. \"\n",
    "        \"Do NOT include emojis, quotes, punctuation, hashtags, URLs, or extra commentary.\\n\\n\"\n",
    "        + \"\\n\".join(cleaned_posts)\n",
    "    )\n",
    "\n",
    "    user_prompt = {\"role\": \"user\", \"content\": prompt}\n",
    "    assistant_prompt = {\"role\": \"assistant\", \"content\": \"Provide a concise topic.\"}\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        [user_prompt, assistant_prompt],\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).to(device)  \n",
    "\n",
    "    outputs = model.generate(chat_prompt, max_new_tokens=max_new_tokens)\n",
    "    label = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    label = re.sub(r'\\s+', ' ', label).strip()\n",
    "    parts = re.split(r'model\\s+Provide a concise topic\\.?\\s*model', label, flags=re.IGNORECASE)\n",
    "    label = parts[-1].strip()\n",
    "    return label\n",
    "\n",
    "\n",
    "# ---------------- Generate cluster summaries ----------------\n",
    "cluster_summary = []\n",
    "rng = np.random.default_rng(seed=42)\n",
    "for i, c in enumerate(cluster_ids):\n",
    "    doc_idx = clusters[c]\n",
    "    if not doc_idx:\n",
    "        continue\n",
    "    print(f\"Cluster: {i}\")\n",
    "\n",
    "\n",
    "    E_cluster = embeddings[doc_idx]\n",
    "    centroid = E_cluster.mean(axis=0)\n",
    "\n",
    "\n",
    "    sim = cosine_similarity(E_cluster, centroid.reshape(1, -1)).ravel()\n",
    "    if sim.sum() == 0:\n",
    "        p = np.ones_like(sim) / len(sim)\n",
    "    else:\n",
    "        p = sim / sim.sum()\n",
    "    sample_size = min(10, len(doc_idx))\n",
    "    random_idx = rng.choice(doc_idx, size=sample_size, replace=False, p=p)\n",
    "    top_posts = [texts_clean[j] for j in random_idx]\n",
    "\n",
    "    label_text = generate_cluster_label(top_posts)\n",
    "\n",
    "\n",
    "    centroid_2d = X_2d_centroids[i]\n",
    "    cluster_summary.append({\n",
    "        \"cluster\": int(c),\n",
    "        \"size\": len(doc_idx),\n",
    "        \"x\": float(centroid_2d[0]),\n",
    "        \"y\": float(centroid_2d[1]),\n",
    "        \"summary\": label_text\n",
    "    })\n",
    "\n",
    "# ---------------- Save cluster summary ----------------\n",
    "with open(os.path.join(OUTPUT_DIR, \"clusters.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cluster_summary, f)\n",
    "\n",
    "print(f\"Saved {len(cluster_summary)} clusters with Gemma 2 labels to {os.path.join(OUTPUT_DIR, 'clusters.pkl')}\")\n",
    "\n",
    "try:\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563b4e7",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02828127",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# ---------------- Load cluster summary ----------------\n",
    "with open(os.path.join(OUTPUT_DIR, \"clusters.pkl\"), \"rb\") as f:\n",
    "    cluster_summary = pickle.load(f)\n",
    "\n",
    "df_plot = pd.DataFrame(cluster_summary)\n",
    "\n",
    "# ---------------- Scale positions to [0,1] for visualization ----------------\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_plot[[\"x\", \"y\"]] = scaler.fit_transform(df_plot[[\"x\", \"y\"]])\n",
    "\n",
    "# ---------------- Scale bubble size for visibility ----------------\n",
    "df_plot[\"size_scaled\"] = np.sqrt(df_plot[\"size\"])\n",
    "\n",
    "# ---------------- Assign random color per cluster ----------------\n",
    "unique_clusters = df_plot[\"cluster\"].unique()\n",
    "colors = px.colors.sample_colorscale(\n",
    "    \"Rainbow\", [i / (len(unique_clusters) - 1) for i in range(len(unique_clusters))]\n",
    ")\n",
    "color_map = dict(zip(unique_clusters, colors))\n",
    "df_plot[\"color\"] = df_plot[\"cluster\"].map(color_map)\n",
    "\n",
    "# ---------------- Add readable label column ----------------\n",
    "df_plot[\"label\"] = df_plot[\"cluster\"].apply(lambda c: f\"Cluster {c}\")\n",
    "\n",
    "# ---------------- Plot with summaries overlayed ----------------\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    size=\"size_scaled\",\n",
    "    color=\"color\",\n",
    "    text=\"summary\",  \n",
    "    hover_data={\"label\": True, \"size\": True, \"cluster\": True},\n",
    "    title=\"Clusters of Posts\",\n",
    "    width=1000,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# ---------------- Improve visualization ----------------\n",
    "fig.update_traces(textposition=\"middle center\", textfont_size=10)\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    showlegend=False  \n",
    ")\n",
    "\n",
    "# ---------------- Use browser renderer to avoid nbformat error ----------------\n",
    "pio.renderers.default = \"browser\"\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
