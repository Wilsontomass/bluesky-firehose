{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18479a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Standard libraries ----------------\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# ---------------- Data handling ----------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# ---------------- Machine learning & NLP ----------------\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "# ---------------- Deep learning & Transformers ----------------\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------- Visualization ----------------\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b486f9",
   "metadata": {},
   "source": [
    "Embedding text from Parquet data frames using Gemma300m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345beb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 17771 posts.\n",
      "After filtering: 16177 posts remain.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0f6c9f5de6471a85fca2791e0b2daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Wrote dataset to directory: output/raw_posts_embeddings_gemma\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Settings ----------------\n",
    "INPUT_PARQUET = \"output/raw_posts_kafka\"\n",
    "BASE_DIR = \"output/raw_posts_embeddings_gemma\"\n",
    "MODEL_ID = \"google/embeddinggemma-300m\"\n",
    "BATCH_SIZE = 256\n",
    "MAX_SEQ_LEN = 128\n",
    "NORMALIZE = False\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# ---------------- NSFW / filtering ----------------\n",
    "NSFW_KEYWORDS = set([\n",
    "    \"porn\",\"sex\",\"nude\",\"sexy\",\"fuck\",\"cock\",\"cum\",\"blowjob\",\n",
    "    \"dick\",\"tits\",\"ass\",\"horny\",\"slut\",\"nsfw\",\"onlyfans\"\n",
    "])\n",
    "\n",
    "MULTILINGUAL_STOPWORDS = set([\n",
    "    \"the\",\"and\",\"a\",\"of\",\"in\",\"to\",\"is\",\"it\",\"for\",\"on\",\n",
    "    \"que\",\"de\",\"le\",\"la\",\"el\",\"en\",\"und\",\"der\",\"die\"\n",
    "])\n",
    "\n",
    "URL_PATTERN = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text_words = [w for w in text.split() if w not in MULTILINGUAL_STOPWORDS]\n",
    "    return \" \".join(text_words)\n",
    "\n",
    "def is_meaningful(text):\n",
    "    if not re.search(r\"\\w\", text):\n",
    "        return False\n",
    "    tokens = text.split()\n",
    "    if len(tokens) == 0:\n",
    "        return False\n",
    "    url_count = sum(bool(URL_PATTERN.match(tok)) for tok in tokens)\n",
    "    if url_count / len(tokens) > 0.5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_nsfw(text):\n",
    "    text_lower = text.lower()\n",
    "    return any(kw in text_lower for kw in NSFW_KEYWORDS)\n",
    "\n",
    "# ---------------- Load posts ----------------\n",
    "df = pd.read_parquet(INPUT_PARQUET, columns=[\"did\", \"rkey\", \"text\"])\n",
    "print(f\"Before filtering: {len(df)} posts.\")\n",
    "\n",
    "df[\"post_id\"] = df[\"did\"].astype(str) + \"/\" + df[\"rkey\"].astype(str)\n",
    "df = df[df[\"text\"].notna()]\n",
    "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "df[\"text_clean\"] = df[\"text\"].apply(preprocess)\n",
    "\n",
    "# ---------------- Preprocessing ----------------\n",
    "df = df[df[\"text_clean\"].apply(is_meaningful)]\n",
    "df = df[~df[\"text_clean\"].apply(is_nsfw)]\n",
    "df = df[df[\"text_clean\"].str.len() > 0]\n",
    "\n",
    "print(f\"After filtering: {len(df)} posts remain.\")\n",
    "\n",
    "# ---------------- Load model ----------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(MODEL_ID, device=device,truncate_dim=256).eval()\n",
    "try:\n",
    "    model.max_seq_length = MAX_SEQ_LEN\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if device == \"cuda\":\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------------- Encode embeddings ----------------\n",
    "texts = df[\"text_clean\"].tolist()\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    prompt_name=\"Clustering\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=False,\n",
    "    convert_to_numpy=True\n",
    ").astype(np.float32)\n",
    "\n",
    "# ---------------- Validate embeddings ----------------\n",
    "finite_mask = np.isfinite(embeddings).all(axis=1)\n",
    "if not finite_mask.all():\n",
    "    n_bad = (~finite_mask).sum()\n",
    "    print(f\"Found {n_bad} rows with NaN/Inf; re-encoding just those rows…\")\n",
    "    bad_idx = np.where(~finite_mask)[0]\n",
    "    bad_texts = [texts[i] for i in bad_idx]\n",
    "\n",
    "    fixed = model.encode(\n",
    "        bad_texts,\n",
    "        batch_size=max(64, BATCH_SIZE // 2),\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=False,\n",
    "        convert_to_numpy=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    embeddings[bad_idx] = fixed\n",
    "    assert np.isfinite(embeddings).all(), \"Still found NaN/Inf after repair\"\n",
    "\n",
    "if NORMALIZE:\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    nz = norms.squeeze() > 0\n",
    "    embeddings[nz] = embeddings[nz] / norms[nz]\n",
    "\n",
    "# ---------------- Save embeddings ----------------\n",
    "shutil.rmtree(BASE_DIR, ignore_errors=True)\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "emb_list_arrays = [pa.array(row, type=pa.float32()) for row in embeddings]\n",
    "table = pa.table({\n",
    "    \"post_id\": pa.array(df[\"post_id\"].tolist()),\n",
    "    \"text\": pa.array(df[\"text\"].tolist()),\n",
    "    \"text_clean\": pa.array(df[\"text_clean\"].tolist()),\n",
    "    \"embedding\": pa.array(emb_list_arrays, type=pa.list_(pa.float32()))\n",
    "})\n",
    "\n",
    "fmt = ds.ParquetFileFormat()\n",
    "opts = fmt.make_write_options(compression=os.environ.get(\"PARQUET_COMPRESSION\", \"zstd\"))\n",
    "\n",
    "ds.write_dataset(\n",
    "    data=table,\n",
    "    base_dir=BASE_DIR,\n",
    "    format=fmt,\n",
    "    file_options=opts,\n",
    "    existing_data_behavior=\"overwrite_or_ignore\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Wrote dataset to directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebbbda",
   "metadata": {},
   "source": [
    "Cluster the posts and prep for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings...\n",
      "Normalizing Embeddings...\n",
      "Dimensionality reduction...\n",
      "Embedding Dimension ---> 64D...\n",
      "HDBSCAN clustering...\n",
      "Number of embeddings: 16177\n",
      "HDBSCAN clustering done in 4.21 seconds\n",
      "Found 118 clusters\n",
      "Reducing cluster centroids to 2D...\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",message=\"n_jobs value .* overridden to 1 by setting random_state\")\n",
    "BASE_DIR = \"output/raw_posts_embeddings_gemma\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# ---------------- Load embeddings ----------------\n",
    "print(\"Loading Embeddings...\")\n",
    "dataset = ds.dataset(BASE_DIR, format=\"parquet\")\n",
    "table = dataset.to_table(columns=[\"post_id\", \"text\", \"text_clean\", \"embedding\"])\n",
    "\n",
    "post_id = table.column(\"post_id\").to_pylist()\n",
    "texts = table.column(\"text\").to_pylist()\n",
    "texts_clean = table.column(\"text_clean\").to_pylist()\n",
    "embeddings = np.vstack(table.column(\"embedding\").to_pylist()).astype(np.float32)\n",
    "\n",
    "# Normalize embeddings\n",
    "print(\"Normalizing Embeddings...\")\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "nonzero = norms.squeeze() > 0\n",
    "embeddings[nonzero] /= norms[nonzero]\n",
    "\n",
    "#remove comment below if you want further dim reduction before clustering or not\n",
    "\"\"\"\n",
    "# ---------------- Dimensionality reduction ----------------\n",
    "\n",
    "print(\"Dimensionality reduction...\")\n",
    "print(\"Embedding Dimension ---> 64D...\")\n",
    "start_time = time.time()\n",
    "reducer_high = umap.UMAP(random_state=2025,n_components=64, metric=\"cosine\")\n",
    "X_64d = reducer_high.fit_transform(embeddings)\n",
    "end_time = time.time()\n",
    "print(f\"Dimensionality reduction done in {end_time - start_time:.2f} seconds\")\n",
    "embeddings = X_64d\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- HDBSCAN clustering ---------------- \n",
    "print(\"HDBSCAN clustering...\")\n",
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "start_time = time.time()\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10,metric=\"euclidean\",cluster_selection_method=\"eom\")\n",
    "labels = clusterer.fit_predict(embeddings) \n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"HDBSCAN clustering done in {end_time - start_time:.2f} seconds\")\n",
    "# ---------------- Group clusters ----------------\n",
    "clusters = defaultdict(list)\n",
    "for i, c in enumerate(labels):\n",
    "    if c == -1:\n",
    "        continue\n",
    "    clusters[c].append(i)\n",
    "\n",
    "print(f\"Found {len(clusters)} clusters\")\n",
    "# ---------------- Compute cluster centroids in embedding space ----------------\n",
    "cluster_ids = []\n",
    "cluster_centroids = []\n",
    "\n",
    "for c, idxs in clusters.items():\n",
    "    cluster_ids.append(c)\n",
    "    cluster_centroids.append(embeddings[idxs].mean(axis=0))\n",
    "\n",
    "cluster_centroids = np.vstack(cluster_centroids)\n",
    "\n",
    "# ---------------- Reduce centroids to 2D for visualization ----------------\n",
    "print(\"Reducing cluster centroids to 2D...\")\n",
    "reducer_2d = umap.UMAP(random_state=2025,n_components=2,metric=\"cosine\",  min_dist=0.1,spread=1.0,)\n",
    "X_2d_centroids = reducer_2d.fit_transform(cluster_centroids)\n",
    "# Map back to clusters for plotting\n",
    "cluster_positions = {c: X_2d_centroids[i] for i, c in enumerate(cluster_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd89c98",
   "metadata": {},
   "source": [
    "Create labels for clusters using GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95dfde41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cluster labels with Gemma 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1b5fd5405847b2b9393427172efcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved 118 clusters with Gemma 2 labels to output\\clusters.pkl\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Gemma 2 Cluster Labeling ----------------\n",
    "print(\"Generating cluster labels with Gemma 2...\")\n",
    "model_name = \"google/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model = model.to(\"cuda\")\n",
    "device = model.device\n",
    "def clean_post(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)     # normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def generate_cluster_label(posts, max_new_tokens=16):\n",
    "    cleaned_posts = [clean_post(p) for p in posts[:5]]\n",
    "\n",
    "    prompt = (\n",
    "        \"Summarize the following posts into 1 concise topic of 3–5 words in English. \"\n",
    "        \"Do NOT include emojis, quotes, punctuation, hashtags, URLs, or extra commentary.\\n\\n\"\n",
    "        + \"\\n\".join(cleaned_posts)\n",
    "    )\n",
    "\n",
    "    user_prompt = {\"role\": \"user\", \"content\": prompt}\n",
    "    assistant_prompt = {\"role\": \"assistant\", \"content\": \"Provide a concise topic.\"}\n",
    "\n",
    "    # Returns a single tensor\n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        [user_prompt, assistant_prompt],\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).to(device)  # single tensor\n",
    "\n",
    "    outputs = model.generate(chat_prompt, max_new_tokens=max_new_tokens)\n",
    "    label = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    label = re.sub(r'\\s+', ' ', label).strip()\n",
    "    parts = re.split(r'model\\s+Provide a concise topic\\.?\\s*model', label, flags=re.IGNORECASE)\n",
    "    label = parts[-1].strip()\n",
    "    return label\n",
    "\n",
    "\n",
    "# ---------------- Generate cluster summaries ----------------\n",
    "cluster_summary = []\n",
    "\n",
    "for i, c in enumerate(cluster_ids):\n",
    "    doc_idx = clusters[c]\n",
    "    if not doc_idx:\n",
    "        continue\n",
    "\n",
    "    E_cluster = embeddings[doc_idx]\n",
    "    centroid_hd = E_cluster.mean(axis=0)\n",
    "    sim = cosine_similarity(E_cluster, centroid_hd.reshape(1, -1)).ravel()\n",
    "    top_idx = np.argsort(-sim)[:3]\n",
    "    top_posts = [texts_clean[doc_idx[j]] for j in top_idx]\n",
    "\n",
    "    \n",
    "    label_text = generate_cluster_label(top_posts)\n",
    "    \n",
    "\n",
    "    centroid_2d = X_2d_centroids[i]\n",
    "    cluster_summary.append({\n",
    "        \"cluster\": int(c),\n",
    "        \"size\": len(doc_idx),\n",
    "        \"x\": float(centroid_2d[0]),\n",
    "        \"y\": float(centroid_2d[1]),\n",
    "        \"summary\": label_text\n",
    "    })\n",
    "\n",
    "# ---------------- Save cluster summary ----------------\n",
    "with open(os.path.join(OUTPUT_DIR, \"clusters.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(cluster_summary, f)\n",
    "\n",
    "print(f\"✓ Saved {len(cluster_summary)} clusters with Gemma 2 labels to {os.path.join(OUTPUT_DIR, 'clusters.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563b4e7",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02828127",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# ---------------- Load cluster summary ----------------\n",
    "with open(os.path.join(OUTPUT_DIR, \"clusters.pkl\"), \"rb\") as f:\n",
    "    cluster_summary = pickle.load(f)\n",
    "\n",
    "df_plot = pd.DataFrame(cluster_summary)\n",
    "\n",
    "# ---------------- Scale positions to [0,1] for visualization ----------------\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_plot[[\"x\", \"y\"]] = scaler.fit_transform(df_plot[[\"x\", \"y\"]])\n",
    "\n",
    "# ---------------- Scale bubble size for visibility ----------------\n",
    "df_plot[\"size_scaled\"] = np.sqrt(df_plot[\"size\"])\n",
    "\n",
    "# ---------------- Assign random color per cluster ----------------\n",
    "unique_clusters = df_plot[\"cluster\"].unique()\n",
    "colors = px.colors.sample_colorscale(\n",
    "    \"Rainbow\", [i / (len(unique_clusters) - 1) for i in range(len(unique_clusters))]\n",
    ")\n",
    "color_map = dict(zip(unique_clusters, colors))\n",
    "df_plot[\"color\"] = df_plot[\"cluster\"].map(color_map)\n",
    "\n",
    "# ---------------- Add readable label column ----------------\n",
    "df_plot[\"label\"] = df_plot[\"cluster\"].apply(lambda c: f\"Cluster {c}\")\n",
    "\n",
    "# ---------------- Plot with summaries overlayed ----------------\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    size=\"size_scaled\",\n",
    "    color=\"color\",\n",
    "    text=\"summary\",  # overlay AI-generated summaries\n",
    "    hover_data={\"label\": True, \"size\": True, \"cluster\": True},\n",
    "    title=\"Clusters of Posts\",\n",
    "    width=1000,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "# ---------------- Improve visualization ----------------\n",
    "fig.update_traces(textposition=\"middle center\", textfont_size=10)\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    showlegend=False  \n",
    ")\n",
    "\n",
    "# ---------------- Use browser renderer to avoid nbformat error ----------------\n",
    "pio.renderers.default = \"browser\"\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
