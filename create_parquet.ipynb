{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8997b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e434a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_SERVERS = \"wilsoniumite.com:9092\"\n",
    "KAFKA_TOPIC = \"bluesky-firehose\"\n",
    "SASL_USERNAME = \"notebook1\"\n",
    "SASL_PASSWORD = \"thisiisatesttoseeifkafkaworks\"  # insert password here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7fcdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/18 15:38:00 WARN Utils: Your hostname, Tomass-Desktop, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/18 15:38:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/wilso/github/bluesky-streaming/venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-561d13a6-75d6-4f32-9a07-4d0b3dce0a9a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 306ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-561d13a6-75d6-4f32-9a07-4d0b3dce0a9a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/6ms)\n",
      "25/10/18 15:38:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark initialized\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BlueskyKafkaToEmbedding\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1\") \\\n",
    "    .master(\"local[48]\")  \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.default.parallelism\", \"400\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.streaming.kafka.maxRatePerPartition\", \"50000\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"256MB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✓ Spark initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1845b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PARQUET = \"output/raw_posts_kafka\"\n",
    "CHECKPOINT_DIR = \"output/kafka_checkpoint\"\n",
    "\n",
    "COLLECTION_DURATION = 20\n",
    "MAX_OFFSETS_PER_TRIGGER = 200000\n",
    "\n",
    "\n",
    "def create_kafka_stream_and_save():\n",
    "    \"\"\"Create Kafka stream/batch read, extract posts, and save to parquet.\"\"\"\n",
    "    \n",
    "    # Define schema for Bluesky data\n",
    "    schema = StructType([\n",
    "        StructField(\"did\", StringType(), True),\n",
    "        StructField(\"time_us\", LongType(), True),\n",
    "        StructField(\"kind\", StringType(), True),\n",
    "        StructField(\"commit\", StructType([\n",
    "            StructField(\"operation\", StringType(), True),\n",
    "            StructField(\"collection\", StringType(), True),\n",
    "            StructField(\"rkey\", StringType(), True),\n",
    "            StructField(\"record\", StructType([\n",
    "                StructField(\"text\", StringType(), True),\n",
    "                StructField(\"langs\", ArrayType(StringType()), True),\n",
    "                StructField(\"createdAt\", StringType(), True)\n",
    "            ]), True)\n",
    "        ]), True)\n",
    "    ])\n",
    "    \n",
    "    # Base Kafka options\n",
    "    kafka_options = {\n",
    "        \"kafka.bootstrap.servers\": KAFKA_SERVERS,\n",
    "        \"subscribe\": KAFKA_TOPIC,\n",
    "        \"kafka.security.protocol\": \"SASL_PLAINTEXT\",\n",
    "        \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "        \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{SASL_USERNAME}\" password=\"{SASL_PASSWORD}\";'\n",
    "    }\n",
    "    \n",
    "    # STREAMING MODE - Memory-safe processing with controlled batch sizes\n",
    "    print(f\"Using STREAMING mode with {COLLECTION_DURATION}s timeout...\")\n",
    "    print(f\"Max {MAX_OFFSETS_PER_TRIGGER} records per micro-batch\")\n",
    "    kafka_options[\"startingOffsets\"] = \"earliest\"\n",
    "    kafka_options[\"maxOffsetsPerTrigger\"] = str(MAX_OFFSETS_PER_TRIGGER)\n",
    "    \n",
    "    # Read from Kafka as stream\n",
    "    kafka_df = spark.readStream.format(\"kafka\")\n",
    "    for key, value in kafka_options.items():\n",
    "        kafka_df = kafka_df.option(key, value)\n",
    "    kafka_df = kafka_df.load()\n",
    "    \n",
    "    # Parse JSON\n",
    "    parsed_df = kafka_df.select(\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "    \n",
    "    # Filter for posts only\n",
    "    posts_df = parsed_df.filter(\n",
    "        (col(\"kind\") == \"commit\") & \n",
    "        (col(\"commit.operation\") == \"create\") &\n",
    "        (col(\"commit.collection\") == \"app.bsky.feed.post\")\n",
    "    ).select(\n",
    "        col(\"did\"),\n",
    "        col(\"commit.rkey\").alias(\"rkey\"),\n",
    "        col(\"commit.record.text\").alias(\"text\"),\n",
    "        col(\"commit.record.langs\").alias(\"langs\"),\n",
    "        col(\"commit.record.createdAt\").alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting data collection from earliest offset...\")\n",
    "    print(f\"Saving to: {OUTPUT_PARQUET}\")\n",
    "    \n",
    "    query = posts_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", OUTPUT_PARQUET) \\\n",
    "        .option(\"checkpointLocation\", CHECKPOINT_DIR) \\\n",
    "        .trigger(processingTime=\"5 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    # Monitor progress\n",
    "    start_time = time.time()\n",
    "    while query.isActive and (time.time() - start_time) < COLLECTION_DURATION:\n",
    "        time.sleep(5)\n",
    "        progress = query.lastProgress\n",
    "        if progress:\n",
    "            num_rows = progress.get(\"numInputRows\", 0)\n",
    "            if num_rows > 0:\n",
    "                print(f\"Processed {num_rows} rows in last batch...\")\n",
    "    \n",
    "    query.stop()\n",
    "    \n",
    "    print(f\"✓ Data collection complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7478a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using STREAMING mode with 20s timeout...\n",
      "Max 200000 records per micro-batch\n",
      "Starting data collection from earliest offset...\n",
      "Saving to: output/raw_posts_kafka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/18 15:38:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/10/18 15:38:15 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000} milliseconds, but spent 10170 milliseconds\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200000 rows in last batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/18 15:38:22 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000} milliseconds, but spent 7181 milliseconds\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200000 rows in last batch...\n",
      "✓ Data collection complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/18 15:38:25 ERROR FileFormatWriter: Aborting job 69959fbb-1760-4aba-9c40-325285a42865.\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:243)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:104)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:374)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:998)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$1(FileFormatWriter.scala:240)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:211)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:176)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:879)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "25/10/18 15:38:25 WARN DAGScheduler: Failed to cancel job group ba02535f-e45c-450a-b11c-e049af40a596. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "create_kafka_stream_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd489a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COLLECTION SUMMARY\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/18 15:38:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:276)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$6.hasNext(Iterator.scala:477)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:110)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:406)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:251)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/10/18 15:38:25 WARN Utils: Suppressing exception in catch: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/wilso/github/bluesky-streaming/output/raw_posts_kafka. SQLSTATE: 58030\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/wilso/github/bluesky-streaming/output/raw_posts_kafka. SQLSTATE: 58030\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:772)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.enrichWriteError(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:135)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1334)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:251)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.nio.channels.ClosedByInterruptException\n",
      "\tat java.base/java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:199)\n",
      "\tat java.base/java.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:465)\n",
      "\tat org.apache.parquet.bytes.BytesInput$ByteBufferBytesInput.writeAllTo(BytesInput.java:700)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:676)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.writeColumnChunk(ParquetFileWriter.java:1314)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.writeColumnChunk(ParquetFileWriter.java:1262)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:408)\n",
      "\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:675)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:210)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:134)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:223)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:80)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$abort$2(FileFormatDataWriter.scala:136)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.enrichWriteError(FileFormatDataWriter.scala:84)\n",
      "\t... 16 more\n",
      "25/10/18 15:38:25 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 2 cancelled Query [id = 6c947204-f7ba-4456-867b-10be96a44bac, runId = ba02535f-e45c-450a-b11c-e049af40a596] was stopped SQLSTATE: XXKDA)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Streaming mode complete\n",
      "Total posts collected: 376,192\n",
      "Collection time: 20 seconds\n",
      "Throughput: 18809.6 posts/second\n",
      "Estimated data size: ~367.4 MB\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check how many posts we collected\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLLECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "df_check = spark.read.parquet(OUTPUT_PARQUET)\n",
    "post_count = df_check.count()\n",
    "print(f\"✓ Streaming mode complete\")\n",
    "print(f\"Total posts collected: {post_count:,}\")\n",
    "print(f\"Collection time: {COLLECTION_DURATION} seconds\")\n",
    "print(f\"Throughput: {post_count / COLLECTION_DURATION:.1f} posts/second\")\n",
    "\n",
    "# Estimate data size\n",
    "estimated_size_mb = post_count * 1024 / (1024 * 1024)  # Assume ~1KB per post\n",
    "print(f\"Estimated data size: ~{estimated_size_mb:.1f} MB\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
