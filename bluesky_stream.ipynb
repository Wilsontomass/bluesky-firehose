{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5738f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fa464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kafka_stream(spark: SparkSession, kafka_servers: str, kafka_topic: str,\n",
    "                       sasl_username: str = None, sasl_password: str = None):\n",
    "    \"\"\"Create a Spark Structured Streaming DataFrame from Kafka.\"\"\"\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"did\", StringType(), True),\n",
    "        StructField(\"time_us\", LongType(), True),\n",
    "        StructField(\"kind\", StringType(), True),\n",
    "        StructField(\"commit\", StructType([\n",
    "            StructField(\"operation\", StringType(), True),\n",
    "            StructField(\"collection\", StringType(), True),\n",
    "            StructField(\"record\", StructType([\n",
    "                StructField(\"text\", StringType(), True),\n",
    "                StructField(\"langs\", ArrayType(StringType()), True),\n",
    "                StructField(\"createdAt\", StringType(), True)\n",
    "            ]), True)\n",
    "        ]), True)\n",
    "    ])\n",
    "    \n",
    "    # Base Kafka options\n",
    "    kafka_options = {\n",
    "        \"kafka.bootstrap.servers\": kafka_servers,\n",
    "        \"subscribe\": kafka_topic,\n",
    "        \"startingOffsets\": \"latest\"\n",
    "    }\n",
    "    \n",
    "    # Add SASL authentication if credentials provided\n",
    "    if sasl_username and sasl_password:\n",
    "        kafka_options.update({\n",
    "            \"kafka.security.protocol\": \"SASL_PLAINTEXT\",  # or \"SASL_SSL\"\n",
    "            \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "            \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{sasl_username}\" password=\"{sasl_password}\";'\n",
    "        })\n",
    "    \n",
    "    # Read from Kafka\n",
    "    kafka_df = spark.readStream \\\n",
    "        .format(\"kafka\")\n",
    "    \n",
    "    for key, value in kafka_options.items():\n",
    "        kafka_df = kafka_df.option(key, value)\n",
    "    \n",
    "    kafka_df = kafka_df.load()\n",
    "    \n",
    "    # Parse JSON\n",
    "    parsed_df = kafka_df.select(\n",
    "        from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    "    ).select(\"data.*\")\n",
    "    \n",
    "    return parsed_df\n",
    "\n",
    "\n",
    "def process_language_counts(df):\n",
    "    \"\"\"Process and aggregate language counts from posts.\"\"\"\n",
    "    \n",
    "    # Filter for posts only\n",
    "    posts_df = df.filter(\n",
    "        (col(\"kind\") == \"commit\") & \n",
    "        (col(\"commit.operation\") == \"create\") &\n",
    "        (col(\"commit.collection\") == \"app.bsky.feed.post\")\n",
    "    )\n",
    "    \n",
    "    # Extract and explode languages\n",
    "    languages_df = posts_df \\\n",
    "        .select(\n",
    "            col(\"commit.record.text\").alias(\"text\"),\n",
    "            explode(coalesce(col(\"commit.record.langs\"), array())).alias(\"language\")\n",
    "        ) \\\n",
    "        .filter(col(\"language\").isNotNull())\n",
    "    \n",
    "    # Add timestamp for windowing\n",
    "    languages_df = languages_df \\\n",
    "        .withColumn(\"timestamp\", current_timestamp())\n",
    "    \n",
    "    # Aggregate by language with 2-minute tumbling window\n",
    "    language_counts = languages_df \\\n",
    "        .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"2 minutes\"),\n",
    "            col(\"language\")\n",
    "        ) \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"post_count\")\n",
    "        ) \\\n",
    "        .select(\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            col(\"language\"),\n",
    "            col(\"post_count\")\n",
    "        ) \\\n",
    "        .orderBy(col(\"post_count\").desc())\n",
    "    \n",
    "    return language_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_SERVERS = \"wilsoniumite.com:9092\"\n",
    "KAFKA_TOPIC = \"bluesky-firehose\"\n",
    "SASL_USERNAME = \"notebook1\"\n",
    "SASL_PASSWORD = \"\"  # ask wilson for this i ain't pushing the password to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09413b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark initialized\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"BlueskyKafkaAnalysis\").config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✓ Spark initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631a826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Kafka stream created\n"
     ]
    }
   ],
   "source": [
    "stream_df = create_kafka_stream(\n",
    "    spark, \n",
    "    KAFKA_SERVERS, \n",
    "    KAFKA_TOPIC, \n",
    "    SASL_USERNAME, \n",
    "    SASL_PASSWORD\n",
    ")\n",
    "print(\"✓ Kafka stream created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e1083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Query started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 15:46:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d3a6de11-4b92-4aaf-84d8-c8405caf8c0f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/15 15:46:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "language_counts = process_language_counts(stream_df)\n",
    "\n",
    "query = language_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"language_counts\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✓ Query started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927a1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------+----------+\n",
      "|language|post_count|window_start|window_end|\n",
      "+--------+----------+------------+----------+\n",
      "+--------+----------+------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT language, post_count, window_start, window_end\n",
    "    FROM language_counts \n",
    "    ORDER BY post_count DESC \n",
    "    LIMIT 20\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
